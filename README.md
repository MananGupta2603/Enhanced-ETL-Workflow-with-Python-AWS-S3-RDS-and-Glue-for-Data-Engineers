
# Enhanced ETL Workflow with Python, AWS S3, RDS, and Glue

## ğŸš€ Project Overview

This project demonstrates a scalable, cloud-based ETL (Extract, Transform, Load) pipeline using Python and AWS services including S3, RDS, and Glue. It handles data from multiple formats (CSV, JSON, XML), transforms and standardizes it, stores it securely, and logs all steps for traceability.

---

## ğŸ“‚ Technologies Used

- **Python** â€“ for scripting ETL logic.
- **AWS S3** â€“ for storing raw and processed data.
- **AWS RDS** â€“ for loading transformed data into a relational database.
- **AWS Glue (Optional)** â€“ for schema inference and transformation.
- **Pandas** â€“ for data manipulation.
- **SQLAlchemy** â€“ for database operations.
- **Logging** â€“ for monitoring pipeline activities.

---

## ğŸ“¦ Dataset

Download dataset:
```bash
wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0221EN-SkillsNetwork/labs/module%206/Lab%20-%20Extract%20Transform%20Load/data/source.zip
```


## ğŸ” ETL Flow Overview

### ğŸ“¤ Upload Raw Files to S3
- Upload **CSV**, **JSON**, and **XML** files to the `raw/` folder of your S3 bucket.

### ğŸ” Extract and Transform with Python
- Read and merge files using **Pandas**, `json`, and `xml.etree`.
- Standardize column names and data types.
- Perform unit conversions:
  - **Inches â†’ Meters**
  - **Pounds â†’ Kilograms**
- Handle missing data, remove duplicates, and normalize categorical values.

### ğŸ’¾ Load to AWS
- Save the transformed data as CSV to the `processed/` folder in S3.
- Use **SQLAlchemy** and `pandas.to_sql()` to load data into an **AWS RDS** database (PostgreSQL/MySQL).

### ğŸ“œ Logging
- Log all ETL steps including data counts, timestamps, and exceptions.
- Save logs both **locally** and in the `logs/` folder on **S3**.
